import threading
import time
import streamlit as st
import os
from datetime import datetime
from PIL import Image
from main import MedGuideAI
# import text_to_speech
from speech_module.ffmpeg_decoding import AudioPlayer
from speech_module.tts_mp3_stream import tts_stream
from speech_module.test_streamlit_stt import speech_to_text
import speed_to_text as sp

# Page configuration
st.set_page_config(
    page_title="MedGuide AI",
    page_icon="ğŸ¥",
    layout="centered"
)

# Define fragments
@st.fragment
def playback_controls(message_id: str, message_content: str):
    """Isolated fragment for managing playback controls for a single message."""
    # Get or create player reference in session state
    if 'audio_players' not in st.session_state:
        st.session_state.audio_players = {}
    
    # Initialize player for this message if needed
    if message_id not in st.session_state.audio_players:
        st.session_state.audio_players[message_id] = None
    
    player = st.session_state.audio_players[message_id]
    is_playing = player is not None and player.is_playing()
    
    # Display appropriate button based on playback state
    if is_playing:
        if st.button("â¹ï¸", key=f"stop_{message_id}"):
            # Stop this specific playback
            if player:
                player.stop()
            # Clear player reference
            st.session_state.audio_players[message_id] = None
            st.rerun(scope="fragment")
            # The fragment will automatically rerun and show the Play button
            
        if isinstance(player.playback_thread, threading.Thread) and player.playback_thread.is_alive():
            # time.sleep(0.1)
            # st.rerun(scope="fragment")
            pass
        else:
            st.session_state.audio_players[message_id] = None
        # time.sleep(3)
        # st.rerun(scope="fragment")

    else:
        if st.button("ğŸ”Š", key=f"play_{message_id}"):
            # First stop any active playback (enforce only one playback at a time)
            for msg_id, p in st.session_state.audio_players.items():
                if p is not None:
                    p.stop()
                    st.session_state.audio_players[msg_id] = None

            # Create new player for this message
            player = AudioPlayer()
            # Start playback in background thread
            def playback():
                player.play(tts_stream(message_content))
            
            t = threading.Thread(target=playback, daemon=True)
            st.session_state.audio_players[message_id] = player
            t.start()
            st.rerun(scope="fragment")

# Initialize AI
@st.cache_resource
def load_ai():
    return MedGuideAI()

if "audio_record_bytes" not in st.session_state:
    st.session_state.audio_record_bytes = None

def main():
    # Header
    st.title("ğŸ¥ MedGuide AI")
    st.markdown("### TÆ° váº¥n y táº¿ thÃ´ng minh vá»›i AI")

    # Initialize
    ai = load_ai()

    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'processing' not in st.session_state:
        st.session_state.processing = False

    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
    if 'patient_context' not in st.session_state:
        st.session_state.patient_context = {
            "medical_history": [],
            "medications": [],
            "allergies": [],
            "symptoms_timeline": []
        }
    if 'processing_image' not in st.session_state:
        st.session_state.processing_image = False
    if 'temp_image' not in st.session_state:
        st.session_state.temp_image = None

    # Welcome message - show full intro on first visit, short version afterwards
    if not st.session_state.messages:
        st.info("""
            ğŸ‘‹ **ChÃ o má»«ng Ä‘áº¿n vá»›i MedGuide AI!**
           
            ğŸ—£ï¸ **Báº¡n cÃ³ thá»ƒ:**
            - Há»i vá» triá»‡u chá»©ng, thuá»‘c, xÃ©t nghiá»‡m
            - Upload hÃ¬nh áº£nh Ä‘Æ¡n thuá»‘c hoáº·c káº¿t quáº£ xÃ©t nghiá»‡m Ä‘á»ƒ phÃ¢n tÃ­ch
            - TrÃ² chuyá»‡n liÃªn tá»¥c vá»›i AI Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chi tiáº¿t
           
            ğŸ’¡ **CÃ¡ch sá»­ dá»¥ng nhanh:**
            - Nháº­p cÃ¢u há»i vÃ  nháº¥n Enter Ä‘á»ƒ gá»­i
            - DÃ¹ng nÃºt ğŸ“· Ä‘á»ƒ gá»­i hÃ¬nh áº£nh y táº¿
            - Sá»­ dá»¥ng cÃ¡c cÃ¢u há»i máº«u bÃªn dÆ°á»›i Ä‘á»ƒ báº¯t Ä‘áº§u
            """)
    else:
        st.info("### TÆ° váº¥n y táº¿ thÃ´ng minh vá»›i AI")

    # Display chat history with container for better scrolling
    if st.session_state.messages:
        st.markdown("### ğŸ’¬ Cuá»™c trÃ² chuyá»‡n")

        # Create container for chat messages
        chat_container = st.container()
        with chat_container:
            for message_index, message in enumerate(st.session_state.messages):
                with st.chat_message(message["role"]):
                    st.write(message["content"])

                    # Show image if exists
                    if "image" in message:
                        st.image(message["image"], width=300)

                    # Show classification info
                    if message["role"] == "assistant" and "topic" in message:
                        topic_icons = {
                            'symptoms': 'ğŸ©º',
                            'drug_groups': 'ğŸ’Š',
                            'lab_results': 'ğŸ§ª',
                            'unknown': 'â“'
                        }
                        # st.caption(f"{topic_icons.get(message['topic'], 'â“')} {message['topic']}")

                    # Add audio player for assistant messages
                    if message["role"] == "assistant":
                        playback_controls(str(message_index), message["content"])

    # Show processing indicators right after chat history
    if st.session_state.get('processing', False):
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Äang xá»­ lÃ½..."):
                st.write("ğŸ¤– Äang xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n...")

    if st.session_state.get('processing_image', False):
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Äang phÃ¢n tÃ­ch hÃ¬nh áº£nh..."):
                st.write("ğŸ” Äang phÃ¢n tÃ­ch hÃ¬nh áº£nh y táº¿...")

    # Input section at bottom
    st.markdown("---")

    # Combined input area with image upload
    col1, col2 = st.columns([6, 4])
    audio_bytes_ = st.audio_input("Speak something...", key="audio_recorder")
    if audio_bytes_:
        st.audio(audio_bytes_)
        print(audio_bytes_.size)
        st.session_state.audio_record_bytes = audio_bytes_
    if st.button("Send"):
        if st.session_state.audio_record_bytes is not None:
            
            audio_text =  sp.speech_to_text_raw_bytes(st.session_state.audio_record_bytes)
            
            audio_content = audio_text
            print(audio_content)
            st.session_state.audio_record_bytes = None

            st.session_state.messages.append({
                "role": "user",
                "content":  audio_content
            })

            st.session_state.processing = True
                    
    with col1:
        # Chat input
        user_text = st.chat_input(
            placeholder="Nháº­p cÃ¢u há»i y táº¿... (Enter Ä‘á»ƒ gá»­i)"
        )
        text_submit = bool(user_text)

    with col2:
        # Use dynamic key to clear file uploader after submit
        upload_key = f"file_upload_{st.session_state.get('upload_counter', 0)}"
        uploaded_file = st.file_uploader(
            "ğŸ“·",
            type=['jpg', 'jpeg', 'png'],
            help="Gá»­i hÃ¬nh áº£nh y táº¿ (Ä‘Æ¡n thuá»‘c, xÃ©t nghiá»‡m...)",
            key=upload_key,
            label_visibility="collapsed"
        )
            

    # Show image preview when uploaded
    if uploaded_file and not st.session_state.get('processing_image', False):
        col1, col2 = st.columns([1, 3])

        with col1:
            st.image(uploaded_file, width=120, caption="HÃ¬nh áº£nh Ä‘Ã£ chá»n")

        with col2:
            st.info("ğŸ“· HÃ¬nh áº£nh Ä‘Ã£ chá»n! Nháº­p cÃ¢u há»i hoáº·c nháº¥n Enter Ä‘á»ƒ phÃ¢n tÃ­ch.")

    # Process text input (with or without image)
    if text_submit and (user_text.strip() or uploaded_file):
        # Determine content and processing type
        if uploaded_file:
            # Image with optional text
            user_content = user_text.strip() if user_text.strip() else "PhÃ¢n tÃ­ch hÃ¬nh áº£nh nÃ y"
            st.session_state.messages.append({
                "role": "user",
                "content": user_content,
                "image": uploaded_file
            })
            st.session_state.processing_image = True
            st.session_state.temp_image = uploaded_file
        else:
            # Text only
            st.session_state.messages.append({
                "role": "user",
                "content": user_text
            })
            st.session_state.processing = True
        st.rerun()

    # Handle processing state (background processing)
    if st.session_state.get('processing', False):
        # Get the last user message
        last_user_msg = None
        for msg in reversed(st.session_state.messages):
            if msg["role"] == "user":
                last_user_msg = msg["content"]
                break

        if last_user_msg:
            # Process with AI (no UI here, just processing)
            print("<duypv10 log> last_user_msg: ", last_user_msg)
            result = ai.process_user_query(last_user_msg)

            if "error" in result:
                response = f"âŒ Lá»—i: {result['error']}"
                topic = "error"
            else:
                response = result.get('ai_response', 'KhÃ´ng cÃ³ pháº£n há»“i')
                topic = result.get('topic_classified', 'unknown')

                # Generate audio
                # audio_bytes = text_to_speech.run_audio(response)

            # Add AI response to messages
            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "topic": topic,
                # "audio": audio_bytes
            })

            # Clear processing state
            st.session_state.processing = False
            st.rerun()

    # Clear file uploader after processing
    if (text_submit and uploaded_file) or st.session_state.get('processing_image', False):
        if 'upload_counter' not in st.session_state:
            st.session_state.upload_counter = 0
        st.session_state.upload_counter += 1

    # Handle image processing state (background processing)
    if st.session_state.get('processing_image', False):
        # Process with AI (no UI here, just processing)
        temp_image = st.session_state.temp_image
        temp_image.seek(0)
        response = ai.analyze_medical_image(temp_image, "general")

        # Generate audio
        # audio_bytes = text_to_speech.run_audio(response)

        # Add AI response to messages
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "topic": "image_analysis",
            # "audio": audio_bytes
        })

        # Clear processing state and file uploader
        st.session_state.processing_image = False
        st.session_state.temp_image = None
        if 'upload_counter' not in st.session_state:
            st.session_state.upload_counter = 0
        st.session_state.upload_counter += 1

        st.rerun()

    # File upload section for Pinecone DB
    with st.sidebar:
        st.markdown("### ğŸ“ ThÃªm tÃ i liá»‡u y táº¿")

        # Collection selection
        collection_choice = st.selectbox(
            "Chá»n loáº¡i tÃ i liá»‡u:",
            ["Tá»± Ä‘á»™ng phÃ¢n loáº¡i", "Triá»‡u chá»©ng", "Thuá»‘c", "XÃ©t nghiá»‡m"],
            help="AI sáº½ tá»± Ä‘á»™ng phÃ¢n loáº¡i hoáº·c báº¡n cÃ³ thá»ƒ chá»n trÆ°á»›c"
        )

        # File uploader
        doc_file = st.file_uploader(
            "Upload file (.txt, .pdf, .docx):",
            type=['txt', 'pdf', 'docx'],
            help="TÃ i liá»‡u y táº¿ Ä‘á»ƒ bá»• sung cÆ¡ sá»Ÿ dá»¯ liá»‡u"
        )

        if doc_file and st.button("ğŸ“¤ ThÃªm vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u", use_container_width=True):
            with st.spinner("Äang xá»­ lÃ½ tÃ i liá»‡u..."):
                try:
                    # Read file content
                    if doc_file.type == "text/plain":
                        content = str(doc_file.read(), "utf-8")
                    else:
                        st.error("Hiá»‡n táº¡i chá»‰ há»— trá»£ file .txt")
                        content = None

                    if content:
                        # Process with Pinecone DB
                        if collection_choice == "Tá»± Ä‘á»™ng phÃ¢n loáº¡i":
                            additions = ai.pinecone_db.add_file_content_to_db(content, doc_file.name)
                        else:
                            # Manual classification
                            collection_map = {
                                "Triá»‡u chá»©ng": "symptoms",
                                "Thuá»‘c": "drug_groups",
                                "XÃ©t nghiá»‡m": "lab_results"
                            }
                            target_collection = collection_map[collection_choice]
                            additions = ai.pinecone_db.add_to_specific_collection(content, doc_file.name, target_collection)

                        # Check for errors
                        if "error" in additions:
                            st.error(f"âŒ Lá»—i khi thÃªm dá»¯ liá»‡u: {additions['error']}")
                            if "No Pinecone connection" in additions['error']:
                                st.warning("âš ï¸ Vui lÃ²ng táº¡o file .env vá»›i PINECONE_API_KEY cá»§a báº¡n")
                        elif sum(additions.values()) == 0:
                            st.warning("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c thÃªm vÃ o. Kiá»ƒm tra ná»™i dung file vÃ  káº¿t ná»‘i Pinecone.")
                        else:
                            st.success(f"âœ… ÄÃ£ thÃªm: {additions}")

                        # Show collection stats
                        stats = ai.pinecone_db.get_collection_stats()
                        st.info(f"ğŸ“Š Tá»•ng: Triá»‡u chá»©ng({stats['symptoms']}), Thuá»‘c({stats['drug_groups']}), XN({stats['lab_results']})")

                except Exception as e:
                    st.error(f"âŒ Lá»—i: {str(e)}")

        st.markdown("---")

    # # Quick actions - always show for easy access  
    # st.markdown("### ğŸš€ CÃ¢u há»i máº«u:")
    # col1, col2, col3 = st.columns(3)

    # with col1:
    #     if st.button("ğŸ’Š Há»i vá» thuá»‘c"):
    #         st.session_state.messages.append({
    #             "role": "user",
    #             "content": "Paracetamol cÃ³ tÃ¡c dá»¥ng gÃ¬?"
    #         })
    #         st.rerun()

    # with col2:
    #     if st.button("ğŸ§ª Há»i vá» xÃ©t nghiá»‡m"):
    #         st.session_state.messages.append({
    #             "role": "user",
    #             "content": "Glucose 150 mg/dL cÃ³ cao khÃ´ng?"
    #         })
    #         st.rerun()

    # with col3:
    #     if st.button("ğŸ©º Há»i vá» triá»‡u chá»©ng"):
    #         st.session_state.messages.append({
    #             "role": "user",
    #             "content": "TÃ´i bá»‹ Ä‘au Ä‘áº§u vÃ  chÃ³ng máº·t"
    #         })
    #         st.rerun()

    # Clear chat
    if st.session_state.messages:
        if st.button("ğŸ—‘ï¸ XÃ³a cuá»™c trÃ² chuyá»‡n", type="secondary"):
            st.session_state.messages = []
            st.rerun()

    # Footer
    st.markdown("---")
    st.caption("âš ï¸ ThÃ´ng tin chá»‰ mang tÃ­nh tham kháº£o, hÃ£y tham kháº£o bÃ¡c sÄ© chuyÃªn khoa")

if __name__ == "__main__":
    main()